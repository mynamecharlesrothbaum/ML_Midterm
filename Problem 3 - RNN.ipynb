{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charles Rothbaum Machine Leaning Midterm - problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this problem we will create a NN to write Shakespeare\n",
    "# plays.\n",
    "# The training data is included in the Training Data \n",
    "# subfolder, and was taken from here:\n",
    "# https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "#\n",
    "# The idea for the project came from this awesome blog post:\n",
    "# https://karpathy.github.io/2015/05/21/rnn-effectiveness/ \n",
    "# I highly recommend you read it. It is great.\n",
    "\n",
    "# Packages\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------- Import Data and Data Preprocessing -------- ###\n",
    "# you must include the appropriate data preprocessing steps\n",
    "\n",
    "text = open('Training Data/3-RNN_input.txt', 'r').read()  # Load the dataset\n",
    "characters = sorted(set(text)) # Create a list in lexi-something of every character in the data.\n",
    "# Map each character to a int representing its location in the characters list3:\n",
    "char_to_int = {}\n",
    "for i, c in enumerate(characters):\n",
    "    char_to_int[c] = i\n",
    "\n",
    "# Convert the whole dataset into integers\n",
    "numerical_text = []\n",
    "for c in text:\n",
    "    numerical_text.append(char_to_int[c])\n",
    "\n",
    "# Break the whole numerical text into sequences:\n",
    "seq_length = 100\n",
    "num_sequences = len(numerical_text) - seq_length\n",
    "\n",
    "#training_sequences = []\n",
    "#next_chars = []\n",
    "\n",
    "#for i in range(0, num_sequences, 1): # iterate from character 0 to 1 seq_length of the end of numerical_text. (because at that point we can no longer extract a label)\n",
    "#    sequence = numerical_text[i:i + seq_length]# slice out a segment that is seq_length long (py slice dont include i+seq_length index)\n",
    "#    next_char = numerical_text[i + seq_length] # the next character after the sequence, which is the label.\n",
    "#    training_sequences.append((sequence, next_char))\n",
    "\n",
    "#print(training_sequences[:5])\n",
    "\n",
    "num_characters = len(characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numerical_text into a 3D array for input sequences where each sequence is a matrix of one-hot encoded vectors. \n",
    "# and 2D array for targets where each target is one-hot encoded vector of next char in sequence.\n",
    "input_sequences = np.zeros((num_sequences, seq_length, num_characters), dtype=bool)\n",
    "targets = np.zeros((num_sequences, num_characters), dtype=bool)\n",
    "\n",
    "for i in range(num_sequences):\n",
    "    for t, char in enumerate(numerical_text[i:i + seq_length]):\n",
    "        input_sequences[i, t, char] = 1\n",
    "    targets[i, numerical_text[i + seq_length]] = 1\n",
    "\n",
    "\n",
    "# Convert to PyTorch Tensors so I can use data loading\n",
    "input_sequences_tensor = torch.tensor(input_sequences, dtype=torch.float32)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.float32)  \n",
    "dataset = torch.utils.data.TensorDataset(input_sequences_tensor, targets_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataLoader with the dataset:\n",
    "\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------ Exploratory Data Analysis ------------- ###\n",
    "# Output two pieces of information that you found \n",
    "# informative as well as a print statement of why they\n",
    "# assisted you in choosing your model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---------------- Model Definition ------------------ ###\n",
    "# Use an LSTM\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)  # input is batch_size x seq_length x features\n",
    "        self.linear = torch.nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length, input_size)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        # Initialize hidden and cell state\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --------- Optimizer and Loss Definition ------------ ###\n",
    "# Output a print statement supporting your optimizer and \n",
    "# loss function choices\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = LSTM(input_size=num_characters, hidden_size=50, num_classes=num_characters)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.148104415577688\n",
      "Epoch 2/10, Loss: 1.8115809551579287\n"
     ]
    }
   ],
   "source": [
    "### ---------------- Training pt I --------------------- ###\n",
    "# Train 10 epochs\n",
    "num_epochs = 10  # Define the number of epochs to train for\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # reset the gradients from the last iteration\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets.argmax(dim=1))  # argmax is what i should use for 1-hot encoding chatgpt told me\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---------------- Testing pt I ---------------------- ###\n",
    "# Write an essay with a minimum of 2,000 characters and \n",
    "# save the output as a PDF named \"RNN_pt1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --------------- Training pt II --------------------- ###\n",
    "# Train an ADDITIONAL 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --------------- Testing pt II ---------------------- ###\n",
    "# Write an essay with a minimum of 2,000 characters and \n",
    "# save the output as a PDF named \"RNN_pt2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------- Training pt III --------------------- ###\n",
    "# Train until you can get it to write a good essay. Take\n",
    "# advantage of the fact that pytorch doesn't reset your model\n",
    "# unless you reinstantiate it in the \"Model Definition\" cell\n",
    "#\n",
    "# If after 3 hours it still doesn't make a meaningful essay\n",
    "# then change some hyperparameters and try again. You can \n",
    "# look to the blog post for hyperparameter inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------- Testing pt III ---------------------- ###\n",
    "# Write an essay with a minimum of 2,000 characters and \n",
    "# save the output as a PDF named \"RNN_pt3.pdf\"\n",
    "#\n",
    "# Output a print statement commenting on wether or not you\n",
    "# enjoyed this problem and why or why not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
